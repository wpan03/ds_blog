<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Logistic Regression’s Coefficients Explanation | fastpages</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Logistic Regression’s Coefficients Explanation" />
<meta name="author" content="Martin Pan" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="An easy to use blogging platform with support for Jupyter Notebooks." />
<meta property="og:description" content="An easy to use blogging platform with support for Jupyter Notebooks." />
<link rel="canonical" href="https://wpan03.github.io/ds_blog/2020/11/20/logistic.html" />
<meta property="og:url" content="https://wpan03.github.io/ds_blog/2020/11/20/logistic.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-11-20T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://wpan03.github.io/ds_blog/2020/11/20/logistic.html","@type":"BlogPosting","headline":"Logistic Regression’s Coefficients Explanation","dateModified":"2020-11-20T00:00:00-06:00","datePublished":"2020-11-20T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://wpan03.github.io/ds_blog/2020/11/20/logistic.html"},"author":{"@type":"Person","name":"Martin Pan"},"description":"An easy to use blogging platform with support for Jupyter Notebooks.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/ds_blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://wpan03.github.io/ds_blog/feed.xml" title="fastpages" /><link rel="shortcut icon" type="image/x-icon" href="/ds_blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/ds_blog/">fastpages</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/ds_blog/about/">About Me</a><a class="page-link" href="/ds_blog/search/">Search</a><a class="page-link" href="/ds_blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Logistic Regression&#39;s Coefficients Explanation</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-11-20T00:00:00-06:00" itemprop="datePublished">
        Nov 20, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Martin Pan</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/wpan03/ds_blog/tree/master/_notebooks/2020-11-20-logistic.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/ds_blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/wpan03/ds_blog/master?filepath=_notebooks%2F2020-11-20-logistic.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/ds_blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/wpan03/ds_blog/blob/master/_notebooks/2020-11-20-logistic.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/ds_blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h3"><a href="#1.-Background">1. Background </a></li>
<li class="toc-entry toc-h3"><a href="#2.-Python-Implementation-of-Logistic-Regression">2. Python Implementation of Logistic Regression </a></li>
<li class="toc-entry toc-h3"><a href="#3.-Interpretation-of-the-Result">3. Interpretation of the Result </a>
<ul>
<li class="toc-entry toc-h4"><a href="#3.1-Prediction">3.1 Prediction </a></li>
<li class="toc-entry toc-h4"><a href="#3.2-Probability,-odd,-log-odd">3.2 Probability, odd, log-odd </a></li>
<li class="toc-entry toc-h4"><a href="#3.3-Coefficient-Explanation">3.3 Coefficient Explanation </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#4.-Estimation-Method">4. Estimation Method </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-11-20-logistic.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This post is going to introduce logistic regression with an emphasis on explaining the coefficients in the output of Python's statsmodels.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1.-Background">
<a class="anchor" href="#1.-Background" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. Background<a class="anchor-link" href="#1.-Background"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Logistic regression is used to solve the classification problem. Classification problem means the dependent variable is categorical. For example, we can build a machine learning model to predict whether a student will be admitted to college based on metrics like his or her GPA, standardized test scores. If we formulate a classification problem in mathematical form, we have:</p>
\begin{equation}
y = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2},
\end{equation}<p>where y is a categorical variable, like whether you are admited or not.</p>
<p>One way to approach this problem is by using linear regression. However, we would like our algorithm will output a number between 0 and 1, which can indicate the probability of an observation belonging to a certain category. Linear regression does not satisfy this requirement as it might output values smaller than 0 or larger than 1.</p>
<p>One solution is to solve this problem is to transform the value of $\beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2}$ to the range of [0,1]. We can do this with the logistic function:
$$f(x)=\frac{e^{x}}{1+e^{x}}$$.</p>
<p>To show that logistic regression will make a number between 0 and 1. Let's make a plot of it.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">logit</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">200</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">logit</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">);</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As we can see, the logit function has a S shape and is bounded by 0 and 1. When x is approaching positive infinity, the logit of x approaches 1. When x is approaching negative infinity, the logit of x approaches 0.</p>
<p>Thus, we can transform the value of $\beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2}$ to:</p>
$$
p(X)=\frac{e^{\beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2}}}{1+e^{\beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2}}}
$$<p>Now the output of our regression will fall between zero and one. Since we the logistic function to achieve this, the regression is called logistic regression. Before jumping into the details of behind it, let's first we how to run it in python.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2.-Python-Implementation-of-Logistic-Regression">
<a class="anchor" href="#2.-Python-Implementation-of-Logistic-Regression" aria-hidden="true"><span class="octicon octicon-link"></span></a>2. Python Implementation of Logistic Regression<a class="anchor-link" href="#2.-Python-Implementation-of-Logistic-Regression"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For this post, we are going to use a college admission dataset from <a href="https://stats.idre.ucla.edu/">UCLA's Institute for Digital Research &amp; Education</a>. We want to use a student's GRE score, GPA, and rank to predict whether he will be admitted.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"https://stats.idre.ucla.edu/stat/data/binary.csv"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>admit</th>
      <th>gre</th>
      <th>gpa</th>
      <th>rank</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>380</td>
      <td>3.61</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>660</td>
      <td>3.67</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>800</td>
      <td>4.00</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>640</td>
      <td>3.19</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>520</td>
      <td>2.93</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We are going to use the statsmodel to run the logistic regression. We choose statsmodel because its interface and summary of the result are more similar to other statistical software, like R and Stata.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">mod</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s1">'admit ~ gre + gpa + rank'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">mod</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>admit</td>      <th>  R-squared:         </th> <td>   0.096</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.089</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   14.02</td>
</tr>
<tr>
  <th>Date:</th>             <td>Fri, 20 Nov 2020</td> <th>  Prob (F-statistic):</th> <td>1.05e-08</td>
</tr>
<tr>
  <th>Time:</th>                 <td>19:47:57</td>     <th>  Log-Likelihood:    </th> <td> -241.53</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   491.1</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   396</td>      <th>  BIC:               </th> <td>   507.0</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P&gt;|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   -0.1824</td> <td>    0.217</td> <td>   -0.841</td> <td> 0.401</td> <td>   -0.609</td> <td>    0.244</td>
</tr>
<tr>
  <th>gre</th>       <td>    0.0004</td> <td>    0.000</td> <td>    2.106</td> <td> 0.036</td> <td> 2.94e-05</td> <td>    0.001</td>
</tr>
<tr>
  <th>gpa</th>       <td>    0.1510</td> <td>    0.063</td> <td>    2.383</td> <td> 0.018</td> <td>    0.026</td> <td>    0.276</td>
</tr>
<tr>
  <th>rank</th>      <td>   -0.1095</td> <td>    0.024</td> <td>   -4.608</td> <td> 0.000</td> <td>   -0.156</td> <td>   -0.063</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>190.649</td> <th>  Durbin-Watson:     </th> <td>   1.950</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>  51.425</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.667</td>  <th>  Prob(JB):          </th> <td>6.81e-12</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 1.858</td>  <th>  Cond. No.          </th> <td>6.00e+03</td>
</tr>
</table>
<br><br>Warnings:<br>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br>[2] The condition number is large,  6e+03. This might indicate that there are<br>strong multicollinearity or other numerical problems.
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.-Interpretation-of-the-Result">
<a class="anchor" href="#3.-Interpretation-of-the-Result" aria-hidden="true"><span class="octicon octicon-link"></span></a>3. Interpretation of the Result<a class="anchor-link" href="#3.-Interpretation-of-the-Result"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="3.1-Prediction">
<a class="anchor" href="#3.1-Prediction" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.1 Prediction<a class="anchor-link" href="#3.1-Prediction"> </a>
</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's make a prediction for a student with GRE 300, GPA 2, and rank 4.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">a_new_student</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We extract the parameters from the logistic regression model to make the prediction.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model_parameters</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">values</span>
<span class="n">model_parameters</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([-0.18241268,  0.00044243,  0.15104023, -0.10950192])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that the following calculation can be made much easier with matrix multiplication with code like <code>a_new_student @ model_parameters</code>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pred</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">pred</span> <span class="o">+=</span> <span class="n">model_parameters</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">a_new_student</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">pred</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>-0.18561215150463872</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that the result is <strong>negative</strong>. Did we say in the previous section the logistic regression guarantees a prediction between 0 and 1? Then why do we get this negative prediction? To understand this, we need to understand the relationship between three terms - probability, odd, and log-odd.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="3.2-Probability,-odd,-log-odd">
<a class="anchor" href="#3.2-Probability,-odd,-log-odd" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.2 Probability, odd, log-odd<a class="anchor-link" href="#3.2-Probability,-odd,-log-odd"> </a>
</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let $k = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \beta_{3}x_{3}$. Then we have: $p(X)=\frac{e^{k}}{1+e^{k}}$, which is the probability of X belonging to a certain class.</p>
<p>Then, $p(x) + p(x)e^k = e^k$. Then $p(x) = e^k - p(x)e^k = e^k(1-p(x))$. Thus, $e^{k} = \frac{p(x)}{1-p(x)}$.
The quantity $\frac{p(x)}{1-p(x)}$ is called the <em>odds</em>.</p>
<p>Taking log from both side the odds equation, we have $k= log(\frac{p(x)}{1-p(x)})$. This quantity $log(\frac{p(x)}{1-p(x)})$ the <em>log-odds</em>.</p>
<p>Bringing back the value of k, we have: $log(\frac{p(x)}{1-p(x)}) =  \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \beta_{3}x_{3}$.</p>
<p>What we are actually evaluating when running the python code in the section is the equation above. Thus, the prediction we made is <strong>not</strong> the probability, but the <strong>log-odds</strong>.</p>
<p>To evaluate the probability, we can put the log-odds we predict back to the formula $p(X)=\frac{e^{k}}{1+e^{k}}$, as k is the log-odd we predict.</p>
<p>In code, we have:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">prob_from_log_odd</span><span class="p">(</span><span class="n">log_odds</span><span class="p">):</span>
    <span class="n">prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_odds</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_odds</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">prob</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Using the prediction we have in section 3.1, we have:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"The log odd is: "</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"The probability is: "</span><span class="p">,</span> <span class="n">prob_from_log_odd</span><span class="p">(</span><span class="n">pred</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>The log odd is:  -0.18561215150463872
The probability is:  0.4537297273635876
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that the probability is about 0.45, which is between 0 and 1. To determine whether the student will be admitted or not, we usually set a 0.5 as the threshold. For students with predicted probability lower than 0.5, we will predict the result as reject. Otherwise, we will predict the result as admit.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="3.3-Coefficient-Explanation">
<a class="anchor" href="#3.3-Coefficient-Explanation" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.3 Coefficient Explanation<a class="anchor-link" href="#3.3-Coefficient-Explanation"> </a>
</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Based on the discussion in section 3.2, the explanation of a specific coefficient of the logistic regression is that given all other variables being the same, how much log-odd will change given one unit increase of the independent variable. For example, the coefficient for gre is about 0.0004, which can be interpreted as given all other variables being the same, one additional GRE score will lead to a 0.0004 increase of the log odd.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="4.-Estimation-Method">
<a class="anchor" href="#4.-Estimation-Method" aria-hidden="true"><span class="octicon octicon-link"></span></a>4. Estimation Method<a class="anchor-link" href="#4.-Estimation-Method"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One question we did not answer so far is that how do we get the estimation of these coefficients for logistic regression. The estimation uses a statistical method called maximum likelihood estimation. The details for this method is beyond the scope of this post. The book <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">The Elements of Statistical Learning</a> gives a detailed discussion of the estimation process in section 4.4. A Python implementation of the estimation from scratch can be found in this <a href="https://beckernick.github.io/logistic-regression-from-scratch/">post</a>.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="wpan03/ds_blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/ds_blog/2020/11/20/logistic.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/ds_blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/ds_blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/ds_blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/ds_blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/ds_blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
