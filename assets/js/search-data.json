{
  
    
        "post0": {
            "title": "Entity Embedding in fastai 2",
            "content": "This post aims to introduce how to use fastai v2 to implement entity embedding for categorical variables in tabular data. Entity embedding is a powerful technique that can sometimes boost the performance of various machine learning methods and reveal the intrinsic properties of categorical variables. See this paper and this post for more details. . import seaborn as sns from fastai.tabular.all import * . Get Data . We will use the California housing dataset for this post. We want to develop a model to predict the median_house_value based on other variables in the dataset. The following code shows the first 5 rows of the dataset. . df = pd.read_csv(&#39;housing.csv&#39;) df.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity . 0 -122.23 | 37.88 | 41.0 | 880.0 | 129.0 | 322.0 | 126.0 | 8.3252 | 452600.0 | NEAR BAY | . 1 -122.22 | 37.86 | 21.0 | 7099.0 | 1106.0 | 2401.0 | 1138.0 | 8.3014 | 358500.0 | NEAR BAY | . 2 -122.24 | 37.85 | 52.0 | 1467.0 | 190.0 | 496.0 | 177.0 | 7.2574 | 352100.0 | NEAR BAY | . 3 -122.25 | 37.85 | 52.0 | 1274.0 | 235.0 | 558.0 | 219.0 | 5.6431 | 341300.0 | NEAR BAY | . 4 -122.25 | 37.85 | 52.0 | 1627.0 | 280.0 | 565.0 | 259.0 | 3.8462 | 342200.0 | NEAR BAY | . Preprocess Data . We rely on various functionalities provided by fastai to preprocess the data. We will not discuss them in detail here. For more information, please check this tutorial. One important note is that when dealing with categorical variables, instead of using one-hot encoding, we map each category into a distinct integer. . cont, cat = cont_cat_split(df, dep_var = &#39;median_house_value&#39;) . splits = RandomSplitter(valid_pct=0.2)(range_of(df)) . to = TabularPandas(df, procs=[Categorify, FillMissing,Normalize], cat_names = cat, cont_names = cont, y_names=&#39;median_house_value&#39;, splits=splits) . dls = to.dataloaders(bs=64) . Train Model . We will train a deep learning model to predict the median housing value and thus get the trained embedding for categorical variables. Again, see this tutorial for details about the meaning of the codes here. . learn = tabular_learner(dls, metrics=rmse) . early_stop_cb = EarlyStoppingCallback(patience=2) . learn.fit_one_cycle(10, cbs=early_stop_cb) . epoch train_loss valid_loss _rmse time . 0 | 55994937344.000000 | 56636186624.000000 | 237983.578125 | 00:04 | . 1 | 56567816192.000000 | 56058937344.000000 | 236767.671875 | 00:04 | . 2 | 52564221952.000000 | 52214001664.000000 | 228503.843750 | 00:04 | . 3 | 45972250624.000000 | 46276349952.000000 | 215119.375000 | 00:04 | . 4 | 37385846784.000000 | 37008117760.000000 | 192374.937500 | 00:04 | . 5 | 31154010112.000000 | 30351321088.000000 | 174216.312500 | 00:04 | . 6 | 24364992512.000000 | 23757260800.000000 | 154133.906250 | 00:04 | . 7 | 22229555200.000000 | 21549965312.000000 | 146799.031250 | 00:04 | . 8 | 20496922624.000000 | 21110308864.000000 | 145293.875000 | 00:04 | . 9 | 20571899904.000000 | 20511488000.000000 | 143218.312500 | 00:04 | . Get Embedding . We will retrieve the trained embedding matrix from the model. . embs = [param for param in learn.model.embeds.parameters()] . The list has two elements and each element represents an embedding matrix for a categorical variable. . len(embs) . 2 . To check what each element corresponds to, we can use the cat_names attributes from TabularPandas. The list indicates that the first element in embs is the embedding matrix for the variable ocean_proximity. . to.cat_names . (#2) [&#39;ocean_proximity&#39;,&#39;total_bedrooms_na&#39;] . Let&#39;s see this matrix. Note that we convert it from a tensor array to a numpy array to make the operation later easier. . ocean_emb = embs[0].detach().numpy() ocean_emb . array([[ 0.00074246, 0.01322438, -0.00539126, 0.00066858], [-0.01550745, -0.02467075, 0.01956671, -0.01148492], [ 0.24266721, 0.35117084, -0.35875005, 0.36369336], [-0.05629548, -0.06088502, 0.08677642, -0.10355063], [-0.01115109, -0.02015582, 0.01277605, -0.00476734], [-0.01796132, -0.02396758, 0.02242134, -0.0143294 ]], dtype=float32) . Each row in the matrix above corresponds to one category in the categorical variable. The categories for ocean_proximity are the following. . cat = to.procs.categorify ocean_cat = cat[&#39;ocean_proximity&#39;] ocean_cat . [&#39;#na#&#39;, &#39;&lt;1H OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN&#39;] . We can use the o2i attribute to see how each category is mapped into an integer. The integer corresponds to the row in the embedding matrix. . ocean_cat.o2i . defaultdict(int, {&#39;#na#&#39;: 0, &#39;&lt;1H OCEAN&#39;: 1, &#39;INLAND&#39;: 2, &#39;ISLAND&#39;: 3, &#39;NEAR BAY&#39;: 4, &#39;NEAR OCEAN&#39;: 5}) . For example, the embedding for &#39;NEAR BAY&#39; is: . ocean_emb[4] . array([-0.01115109, -0.02015582, 0.01277605, -0.00476734], dtype=float32) . Let&#39;s create a dictionary to map each category to its corresponding embedding. . ocean_emb_map = {ocean_cat[i]:ocean_emb[i] for i in range(len(ocean_cat))} . Apply Embedding . This section shows how we can apply the embedding to the original dataset; that is, change the category variable to numeric vectors. . emb_dim = ocean_emb.shape[1] . col_name = [f&#39;ocean_emb_{i}&#39; for i in range(1,emb_dim+1)] df_emb = pd.DataFrame(df[&#39;ocean_proximity&#39;].map(ocean_emb_map).to_list(), columns=col_name) df_emb.head() . ocean_emb_1 ocean_emb_2 ocean_emb_3 ocean_emb_4 . 0 -0.011151 | -0.020156 | 0.012776 | -0.004767 | . 1 -0.011151 | -0.020156 | 0.012776 | -0.004767 | . 2 -0.011151 | -0.020156 | 0.012776 | -0.004767 | . 3 -0.011151 | -0.020156 | 0.012776 | -0.004767 | . 4 -0.011151 | -0.020156 | 0.012776 | -0.004767 | . df_new = pd.concat([df, df_emb],axis=1) . Visualize Embedding . Another way to explore the embedding matrix is to visualize it and see what it learns. We will use principle component analysis(PCA) to visualize the embedding. . from sklearn import decomposition pca = decomposition.PCA(n_components=2) pca_result = pca.fit_transform(ocean_emb) . df_visualize = pd.DataFrame({&#39;name&#39;:ocean_cat, &#39;dim1&#39;:pca_result[:,0], &#39;dim2&#39;: pca_result[:,1]}) sns.scatterplot(data=df_visualize, x=&#39;dim1&#39;, y=&#39;dim2&#39;, hue=&#39;name&#39;); . If we compare this visualization to the map of these houses above, we will find that the embedding matrix does provide some useful insights. In particular, we see that the relative location of the category INLAND does correspond to the inland area in the map. The code to make the following map comes from this notebook. . Ending Note . This is the end of this post. You can see the full code by clicking to the View on Github or Open in Colab tab at the top this post. .",
            "url": "https://wpan03.github.io/ds_blog/2020/12/21/fastai_emb.html",
            "relUrl": "/2020/12/21/fastai_emb.html",
            "date": " • Dec 21, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Logistic Regression's Coefficients Explanation",
            "content": "This post is going to introduce logistic regression with an emphasis on explaining the coefficients in the output of Python&#39;s statsmodels. . import numpy as np import pandas as pd import matplotlib.pyplot as plt . . 1. Background . Logistic regression is used to solve the classification problem. Classification problem means the dependent variable is categorical. For example, we can build a machine learning model to predict whether a student will be admitted to college based on metrics like his or her GPA, standardized test scores. If we formulate a classification problem in mathematical form, we have: . $$y = beta_{0} + beta_{1}x_{1} + beta_{2}x_{2}$$, . where y is a categorical variable, like whether you are admited or not. . One way to approach this problem is by using linear regression. However, we would like our algorithm will output a number between 0 and 1, which can indicate the probability of an observation belonging to a certain category. Linear regression does not satisfy this requirement as it might output values smaller than 0 or larger than 1. . One solution is to solve this problem is to transform the value of $ beta_{0} + beta_{1}x_{1} + beta_{2}x_{2}$ to the range of [0,1]. We can do this with the logistic function: $$f(x)= frac{e^{x}}{1+e^{x}}$$. . To show that logistic regression will make a number between 0 and 1. Let&#39;s make a plot of it. . def logit(x): return np.exp(x)/(1+np.exp(x)) . x = np.arange(-200, 200, 0.1) y = logit(x) plt.plot(x,y); . As we can see, the logit function has a S shape and is bounded by 0 and 1. When x is approaching positive infinity, the logit of x approaches 1. When x is approaching negative infinity, the logit of x approaches 0. . Thus, we can transform the value of $ beta_{0} + beta_{1}x_{1} + beta_{2}x_{2}$ to: . $$ p(X)= frac{e^{ beta_{0} + beta_{1}x_{1} + beta_{2}x_{2}}}{1+e^{ beta_{0} + beta_{1}x_{1} + beta_{2}x_{2}}} $$Now the output of our regression will fall between zero and one. Since we the logistic function to achieve this, the regression is called logistic regression. Before jumping into the details of behind it, let&#39;s first we how to run it in python. . 2. Python Implementation of Logistic Regression . For this post, we are going to use a college admission dataset from UCLA&#39;s Institute for Digital Research &amp; Education. We want to use a student&#39;s GRE score, GPA, and rank to predict whether the student will be admitted. . df = pd.read_csv(&quot;https://stats.idre.ucla.edu/stat/data/binary.csv&quot;) . df.head() . admit gre gpa rank . 0 0 | 380 | 3.61 | 3 | . 1 1 | 660 | 3.67 | 3 | . 2 1 | 800 | 4.00 | 1 | . 3 1 | 640 | 3.19 | 4 | . 4 0 | 520 | 2.93 | 4 | . We are going to use the statsmodel package to run the logistic regression. We choose statsmodel because its interface and summary of the result are more similar to other statistical software, like R and Stata. . import statsmodels.formula.api as smf . mod = smf.ols(formula=&#39;admit ~ gre + gpa + rank&#39;, data=df).fit() . mod.summary() . OLS Regression Results Dep. Variable: admit | R-squared: 0.096 | . Model: OLS | Adj. R-squared: 0.089 | . Method: Least Squares | F-statistic: 14.02 | . Date: Fri, 20 Nov 2020 | Prob (F-statistic): 1.05e-08 | . Time: 19:47:57 | Log-Likelihood: -241.53 | . No. Observations: 400 | AIC: 491.1 | . Df Residuals: 396 | BIC: 507.0 | . Df Model: 3 | | . Covariance Type: nonrobust | | . | coef std err t P&gt;|t| [0.025 0.975] . Intercept -0.1824 | 0.217 | -0.841 | 0.401 | -0.609 | 0.244 | . gre 0.0004 | 0.000 | 2.106 | 0.036 | 2.94e-05 | 0.001 | . gpa 0.1510 | 0.063 | 2.383 | 0.018 | 0.026 | 0.276 | . rank -0.1095 | 0.024 | -4.608 | 0.000 | -0.156 | -0.063 | . Omnibus: 190.649 | Durbin-Watson: 1.950 | . Prob(Omnibus): 0.000 | Jarque-Bera (JB): 51.425 | . Skew: 0.667 | Prob(JB): 6.81e-12 | . Kurtosis: 1.858 | Cond. No. 6.00e+03 | . Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 6e+03. This might indicate that there arestrong multicollinearity or other numerical problems. 3. Interpretation of the Result . 3.1 Prediction . Let&#39;s make a prediction for a student with GRE 300, GPA 2, and rank 4. . a_new_student = np.array([1, 300, 2, 4]) . We extract the parameters from the logistic regression model to make the prediction. . model_parameters = mod.params.values model_parameters . array([-0.18241268, 0.00044243, 0.15104023, -0.10950192]) . Note that the following calculation can be made much easier with matrix multiplication with code like a_new_student @ model_parameters. . pred = 0 for i in range(4): pred += model_parameters[i] * a_new_student[i] pred . -0.18561215150463872 . Note that the result is negative. Did we say in the previous section the logistic regression guarantees a prediction between 0 and 1? Then why do we get this negative prediction? To understand this, we need to understand the relationship between three terms - probability, odd, and log-odd. . 3.2 Probability, odd, log-odd . Let $k = beta_{0} + beta_{1}x_{1} + beta_{2}x_{2} + beta_{3}x_{3}$. As we discussed in section 1, we have: $p(X)= frac{e^{k}}{1+e^{k}}$, which is the probability of X belonging to a certain class. . Then, $p(x) + p(x)e^k = e^k$. Then $p(x) = e^k - p(x)e^k = e^k(1-p(x))$. Thus, $e^{k} = frac{p(x)}{1-p(x)}$. The quantity $ frac{p(x)}{1-p(x)}$ is called the odds. . Taking log from both side the odds equation, we have $k= log( frac{p(x)}{1-p(x)})$. This quantity $log( frac{p(x)}{1-p(x)})$ the log-odds. . Bringing back the value of k, we have: $log( frac{p(x)}{1-p(x)}) = beta_{0} + beta_{1}x_{1} + beta_{2}x_{2} + beta_{3}x_{3}$. . What we are actually evaluating when running the python code in the section is the equation above. Thus, the prediction we made is not the probability, but the log-odds. . To evaluate the probability, we can put the log-odds we predict back to the formula $p(X)= frac{e^{k}}{1+e^{k}}$, as k is the log-odd we predict. . In code, we have: . def prob_from_log_odd(log_odds): prob = np.exp(log_odds)/(1+np.exp(log_odds)) return prob . Using the prediction we have in section 3.1, we have: . print(&quot;The log odd is: &quot;, pred) print(&quot;The probability is: &quot;, prob_from_log_odd(pred)) . The log odd is: -0.18561215150463872 The probability is: 0.4537297273635876 . Note that the probability is about 0.45, which is between 0 and 1. To determine whether the student will be admitted or not, we usually set a 0.5 as the threshold. For students with predicted probability lower than 0.5, we will predict the result as reject. Otherwise, we will predict the result as admit. . 3.3 Coefficient Explanation . Based on the discussion in section 3.2, the explanation of a specific coefficient of the logistic regression is that given all other variables being the same, how much log-odd will change given one unit increase of the independent variable. For example, the coefficient for gre is about 0.0004, which can be interpreted as given all other variables being the same, one additional GRE score will lead to a 0.0004 increase of the log odd. . 4. Estimation Method . One question we did not answer so far is that how do we get the estimation of these coefficients for logistic regression. The estimation uses a statistical method called maximum likelihood estimation. The details for this method is beyond the scope of this post. The book The Elements of Statistical Learning gives a detailed discussion of the estimation process in section 4.4. A Python implementation of the estimation from scratch can be found in this post. .",
            "url": "https://wpan03.github.io/ds_blog/2020/11/20/logistic.html",
            "relUrl": "/2020/11/20/logistic.html",
            "date": " • Nov 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://wpan03.github.io/ds_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://wpan03.github.io/ds_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}