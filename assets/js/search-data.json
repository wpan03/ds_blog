{
  
    
        "post0": {
            "title": "Pydantic",
            "content": "My Understanding of Pydantic . I am recently reading the book Robust Python by Patrick Viafore. In this blog post, I want to discuss the usage of pydantic library, introduced in chapter 14 of the book. . I have heard about pydantic for a long time since its usage in popular libraries like FastAPI. However, I am still confused about the functionality of this library when reading the introduction of documentation. This book helps me understand this topic a lot better. . In short, pydantic can help us validate (or parse) data during the runtime with type checks and avoid using a large amount of if-else conditions to validate the data. You might be as confused as me when you read this sentence for the first time. Let’s walk through an example together. . Suppose we have a deep learning model training script. One of the function inside it requires the user to pass the learning rate and optimizer in a yaml file. You can learn more about yaml file here if you are not familiar with it. . The yaml file might look like the following. . model_config: learning_rate: 0.01 optimizer: adam . A sample function can look like the following: . from typing import Dict import yaml with open(&#39;config.yaml&#39;) as yaml_file: config: Dict = yaml.safe_load(yaml_file) model_config = config[&quot;model_config&quot;] def train_model(model_config: Dict): lr = model_config[&quot;learning_rate&quot;] optimizer = config_file[&quot;optmizer&quot;] # the model training code ... . However, we want to restrict the learning rate to be a float between 0.0 and 1.0, and force optimizer to be one of the following: sgd, rmsprop, adam. Instead of throwing the error when the model begins to train or letting the model fail silently (the code runs but the model cannot converge because of a huge learning rate), we want to catch this error when we load the data from yaml. . One naive solution is to use try-except and if-else statements like the following. . try: lr = float(model_config[&quot;learning_rate&quot;]) except: raise ValueError(&quot;learning rate needs to be a float&quot;) optimizer = str(model_config[&quot;optimizer&quot;]) if lr &lt; 0. or lr &gt; 1.: raise ValueError(&quot;learning rate needs to between 0.0 and 1.0&quot;) if optimizer not in [&quot;sgd&quot;, &quot;rmsprop&quot;, &quot;adam&quot;]: raise ValueError(&quot;optimizer needs to be sgd, rmsprop, or adam&quot;) . This can get extremely long and complex when have a large list of hyper parameters. Also, even we want to use type hint and type checker like mypy to get rid of the checking for type like the following (Note that you need to have Python 3.8 or above to use the Literal type). . from typing import Literal, TypedDict class ModelConfig(TypedDict): learning_rate: float optimizer: Literal[&quot;sgd&quot;, &quot;rmsprop&quot;, &quot;adam&quot;] def load_yaml(file_path: str) -&gt; ModelConfig: with open(file_path) as yaml_file: config = yaml.safe_load(yaml_file) return config[&quot;model_config&quot;] . We are still out of luck because we don’t know the information in the yaml file until we actually run the script (during runtime). Thus, even we have a yaml file like this: . model_config: learning_rate: not_learning optimizer: 1.0 . Type checker like mypy will not throw any error. This is when pydantic comes in. It checks type during the runtime. . from pydantic import BaseModel from pydantic import confloat class ModelConfig(BaseModel): learning_rate: confloat(gt=0., lt=1.) optimizer: Literal[&quot;sgd&quot;, &quot;rmsprop&quot;, &quot;adam&quot;] def load_yaml(file_path: str) -&gt; ModelConfig: with open(file_path) as yaml_file: config = yaml.safe_load(yaml_file) return ModelConfig(**config[&quot;model_config&quot;]) . Let’s explain few things in this code. . We inherit from the BaseModel class of the pydantic model so when we construct this class during the run time, the type checking will be executed. | The confloat type in pydantic allows us to restrict the range of a float value. In our case, we require it to be greater than 0 or less than 1. | We use double asterisk (**) to construct the class from a dictionary. You can learn more about it here. | . Now, if we run the script by passing it the wrong yaml config file above, we will get the following error. . ValidationError: 2 validation errors for ModelConfig learning_rate value is not a valid float (type=type_error.float) optimizer unexpected value; permitted: &#39;sgd&#39;, &#39;rmsprop&#39;, &#39;adam&#39; (type=value_error.const; given=1.0; permitted=(&#39;sgd&#39;, &#39;rmsprop&#39;, &#39;adam&#39;)) . Note that pydantic does the validation for us automatically and simplifies our code quite a bit. To reiterate, pydantic can help us validate (or parse) data during the runtime with type checks and avoid using a large amount of if-else conditions to validate the data. Hopefully now this sentence makes more sense to you. . My understanding of pydantic is still at a basic level and I encourage you to check out the documentation of pydantic to learn more about it. Also, the robust python is a really nice book for introducing typing in Python and many other good coding practices. .",
            "url": "https://wpan03.github.io/ds_blog/2022/01/26/pydantic.html",
            "relUrl": "/2022/01/26/pydantic.html",
            "date": " • Jan 26, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Construct ROC Curve from Scratch",
            "content": "This post aims to show how to construct the receiver operating characteristic (roc) curve without using predefined functions. It hopes to help you better understand how the roc curve is constructed and how to interpret it. . Setup . %matplotlib inline from typing import Tuple, Union import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn import metrics from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split . . Load Data and Train Model . The dataset we will use for this blog is the famous Titanic dataset. For simplicity, we are going to use a subset of the data that does not contain missing values. . data_path = &quot;https://raw.githubusercontent.com/wpan03/quick_ds_python/master/data/titanic_train.csv&quot; sample_size = 200 seed = 36 . . df_titanic = (pd.read_csv(data_path) .dropna(subset=[&#39;Age&#39;]) .sample(sample_size, random_state=seed) .reset_index(drop=True) ) . . This is the first 5 rows of the dataset. . df_titanic.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 690 | 1 | 1 | Madill, Miss. Georgette Alexandra | female | 15.0 | 0 | 1 | 24160 | 211.3375 | B5 | S | . 1 272 | 1 | 3 | Tornquist, Mr. William Henry | male | 25.0 | 0 | 0 | LINE | 0.0000 | NaN | S | . 2 52 | 0 | 3 | Nosworthy, Mr. Richard Cater | male | 21.0 | 0 | 0 | A/4. 39886 | 7.8000 | NaN | S | . 3 516 | 0 | 1 | Walker, Mr. William Anderson | male | 47.0 | 0 | 0 | 36967 | 34.0208 | D46 | S | . 4 666 | 0 | 2 | Hickman, Mr. Lewis | male | 32.0 | 2 | 0 | S.O.C. 14879 | 73.5000 | NaN | S | . We train a simple logistic regression model by using only two columns as the independent variables - Fare and Age and try to predict whether a passenger can survive in the accident. . x_data = df_titanic[[&quot;Fare&quot;, &quot;Age&quot;]] y = df_titanic[&quot;Survived&quot;] x_train, x_test, y_train, y_test = train_test_split( x_data, y, test_size=0.2, random_state=seed ) . mod_lg = LogisticRegression(random_state=seed).fit(x_train, y_train) . Get ROC Curve . With the model setup, we can go into the core steps for constructing the roc curve. Constructing the roc curve includes 4 steps (this is adapted from lecture notes from Professor Spenkuch&#39;s business analytics class). . Sort predicted probability of &quot;positive&quot; outcome for each observation. | For each observation, record false positive rate (fpr) and true positive rate (tpr) if that observation&#39;s predicted probability were used as classification threshold. | Plot recorded pairs of tpr and fpr. | Connect the dots. | Let&#39;s show how to do those step by step. . First, we can get the sorted probability of positive outcomes (prediction == 1) of the next two lines of code. . y_pred_proba = mod_lg.predict_proba(x_test)[:, 1] y_pred_proba_asc = np.sort(y_pred_proba) . Second, we define a function to calculate the fpr and tpr for a given threshold. . def get_tpr_fpr_pair( y_proba: np.ndarray, y_true: Union[np.ndarray, pd.Series], threshold: float ) -&gt; Tuple[float, float]: &quot;&quot;&quot;Get the true positive rate and false positive rate based on a certain threshold&quot;&quot;&quot; y_pred = (y_proba &gt;= threshold).astype(int) tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred).ravel() tpr = tp / (tp + fn) fpr = fp / (fp + tn) return tpr, fpr . With the function defined above, we can loop through each element in the sorted probability of positive outcomes and use each element as the threshold for our get_tpr_fpr_pair function and store the result in two lists. . tpr_list = [] fpr_list = [] for t in y_pred_proba_asc: tpr, fpr = get_tpr_fpr_pair(y_pred_proba, y_test, threshold=t) tpr_list.append(tpr) fpr_list.append(fpr) . Finally, after we have the record for each pair of tpr and fpr, we can plot them to get the roc curve. . plt.scatter(fpr_list, tpr_list) plt.plot(fpr_list, tpr_list) plt.xlabel(&#39;FPR&#39;) plt.ylabel(&#39;TPR&#39;) plt.show() . The following code shows what we construct from scratch is the same as what we get from the predefined functions in scikit-learn. . metrics.RocCurveDisplay.from_estimator(mod_lg, x_test, y_test) plt.show() . That’s it for this blog post. To learn ROC and AUC from another perspective, you can check out this excellent video here. . Appendix . This appendix shows the Python and library version we used when writing this blog. . %load_ext watermark %watermark --iversions . sys : 3.8.8 (default, Feb 24 2021, 13:46:16) [Clang 10.0.0 ] pandas : 1.3.1 numpy : 1.19.2 matplotlib: 3.3.2 sklearn : 1.0 .",
            "url": "https://wpan03.github.io/ds_blog/2021/11/18/roc.html",
            "relUrl": "/2021/11/18/roc.html",
            "date": " • Nov 18, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Notes from Data Science at the Command Line",
            "content": "In this post, I want to write down some ideas from Jeroen Janssens’s book Data Science at the Command Line, 2e. Instead of being a comprehensive overview of the book, this note only picks some ideas that add to my existing knowledge and hopefully it can bring some new thoughts to you. Of course, this blog post reflects my personal understanding and all errors remain mine. . Introduction . The original book can be divided into two parts. The first part covers how to use the command line to do four essential parts of data science: (1) obtaining data, (2) scrubbing data, (3) exploring data, (4) modeling data. The second part of the book introduces some useful command-line tools and methods for doing data science, including: (1) create command-line tools with shell script, Python and R; (2) use Makefile to do project and pipeline management; (3) implement parallel computing in the command line; (4) incorporate command line tools into other programming languages, including Python, R and Spark. . However, to summarize my learning, I will break this blog post into four parts: . Useful tools: some command line packages and tools, including Docker, Make, and few other packages. | Syntax tricks: some useful syntax that can be helpful for daily work. | Integration with Python: how to combine the power of command line tools with Python. | Useful Resources: some interesting resources for further learning. | . Useful Tools . Docker . Docker in itself is a fascinating but complex topic and thus is beyond the scope of this blog and the original book. However, one idea I learn from the book is how the author uses Docker as an educational tool, which allows users to set up the same environment and follow the book easily. After installing Docker, just running two lines of code in the command line will set up the prepared environment from the book. . docker pull datasciencetoolbox/dsatcl2e docker run --rm -it datasciencetoolbox/dsatcl2e . Another trick about Docker I learned is how the author uses the -v flag to get data in and out of container with a command like the following. (In macOS or Linux). . docker run --rm -it -v &quot;$(pwd)&quot;:/data datasciencetoolbox/dsatcl2e . The author recommended another book called Docker: Up &amp; Running to learn more about Docker. You can also see this A Docker Tutorial for Beginners to learn more about Docker. . Make . I was fortunate enough to learn and use make in the past, which is one of the most valuable tools I learned in 2021. If you haven’t learned, you should check out chapter 6 of Data Science at the Command Line, 2e to learn about it. make provides a nice way to build a data science pipeline and simplify complex commands. . I will only highlight few tips I learn from chapter 6 here. . If the configuration file is not called Makefile (usually you shouldn’t do this), you can run still run the file with a -f flag. For example, if you name the configuration file hello.make, I can run it with the following: . make -f hello.make . | You can use the automatic variable $@ to avoid some duplicates. For example, the following two commands will be equivalent. . numbers: seq 7 &gt; $@ numbers: seq 7 &gt; numbers . | We can change the configuration of the make with the following codes in the top of the Makefile . SHELL := bash .SHELLFLAGS := -eu -o pipefail -c . I will directly quote the author to explain this: . All rules are executed in a shell, which by default, is sh. With the SHELL variable we can change this to another shell, like bash. This way we can use everything that Bash has to offer such as for loops. | The .SHELLFLAGS line makes Bash more strict, which is considered a best practice. For example, because of this, the pipeline in the rule for target top10 now stops as soon as there is an error. | | Other tools . There are two command line packages introduced in the book I particularly like: bat and tldr. Note that usually these two tools are not pre-installed and thus you need to install them with instructions from the link. . bat is an enhanced version of the common command-line tool cat. The following picture shows three benefits from bat: . It provides syntax highlighting. | It shows modification information from git (see the red line in line 10). | When showing a long file, instead of overwhelming the terminal with all the information like cat, it pipes the output to a pager like the less command. | . . tldr aims to simplify the man command manual with concrete examples. Consider the following two manuals for the find command. . . . If you are not looking for some advanced and specific usages, the output page from tldr provides simpler and more useful instructions for understanding a command. . Syntax Trick . There are several useful syntaxes I learn from the book. . The following command prints the variable in the variable PATH into separate lines, which can be helpful to debug issues around the environment path. . echo $PATH | tr &#39;:&#39; &#39; n&#39; . | Shebang . From the book: “The shebang is a special line in the script that instructs the system which executable it should use to interpret the commands.” | You can see an example of shebang below. (#!/usr/bin/env python in the Python script). | The author recommends using commands like !/usr/bin/env bash instead of !/usr/bin/bash because this can avoid errors when executables are not in /usr/bin. | . | Integration with Python . Another useful thing I learned is to combine shell commands with Python. Specifically, there are two ways: (1) using shell command in Jupyter; (2) using Python to write shell scripts. . Shell Command in Jupyter . In a Python code shell of jupyter notebook or jupyter lab, we can call use shell commands by adding ! before the command. For example, we can list the file in the current directory with . ! ls . Or even assign the output of the command line to a Python variable. For example, we can do: . file_list = ! ls . This functionality allows us to utilize the brevity of shell commands to achieve things quickly. One example the author gives in the book is about downloading a file from the Internet. . url = &quot;https://www.gutenberg.org/files/11/old/11.txt&quot; # Command line way (curly braces allow us to add python variable) ! curl &#39;{url}&#39; &gt; alice3.txt # Python way import requests with open(&quot;alice2.txt&quot;, &quot;wb&quot;) as f: response = requests.get(url) f.write(response.content) . As we can see, the command line achieves the same thing with less code but still being easy to understand. . Another syntax trick is that if you want to use literal curly braces for command line, you can type it twice in jupyter like the following: . ! rm -v alice{{2,3}}.txt . Use Python to Write Shell Scripts . We can create a shell script with Python and combine it into a command line pipeline. One example the author gives in the book is a script for the Fizz Buzz problem. The script is called fizzbuzz.py. . #!/usr/bin/env python import sys CYCLE_OF_15 = [&quot;fizzbuzz&quot;, None, None, &quot;fizz&quot;, None, &quot;buzz&quot;, &quot;fizz&quot;, None, None, &quot;fizz&quot;, &quot;buzz&quot;, None, &quot;fizz&quot;, None, None] ## See the usage of type hint with :int and -&gt; str def fizz_buzz(n: int) -&gt; str: ## In Python, (None or 1) = 1 (&#39;buzz&#39; or 5) = &#39;buzz&#39; ## if the first element in or is evaluated as ## None or False, it will go to the second element return CYCLE_OF_15[n % 15] or str(n) if __name__ == &quot;__main__&quot;: try: ## See the walrus operator := below ## which allows assign and evaluate at the same time while (n:= sys.stdin.readline()): print(fizz_buzz(int(n))) except: pass . With this script, you can combine it into a command line pipeline with the following command, which will show the result of Fizz Buzz problem for number from 1 to 30. . seq 30 | ./fizzbuzz.py | column -x . Another thing I like is the way the author writes the Python code. In the simple script above, we can learn several Python techniques, including the walrus operator introduced in Python 3.8, type hint for functions, and the usage of or. I wrote my own comment with ## in the command to annotate them. . Useful Resources . The first resource is definitely to read through the book Data Science at the Command Line, 2e, which has a free online version. This blog only reflects my personal learning. Given everyone’s unique background, you are likely to learn something different from the book. In addition to that, the author also recommends several resources inside the book which I find interesting: . Thinking with Data: this book seems to be a friendly introduction to how to approach real-world problems with a data-driven approach | Explain Shell: a useful website that parses a command in the shell and explain the usage of each part | Ten Essays on Fizz Buzz: a book that discusses very aspects of Python and software design more general through 10 solutions for the Fizz Buzz problem | Pro Git: a comprehensive guide to git | Regular Expressions Cookbook: a guide for regular expressions | . I hadn’t read any of the books above in detail but plan to do so and will try to share my notes for them once I finished. .",
            "url": "https://wpan03.github.io/ds_blog/2021/08/19/ds_cmd.html",
            "relUrl": "/2021/08/19/ds_cmd.html",
            "date": " • Aug 19, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Entity Embedding in fastai 2",
            "content": "This post aims to introduce how to use fastai v2 to implement entity embedding for categorical variables in tabular data. Entity embedding is a powerful technique that can sometimes boost the performance of various machine learning methods and reveal the intrinsic properties of categorical variables. See this paper and this post for more details. . import seaborn as sns from fastai.tabular.all import * . Get Data . We will use the California housing dataset for this post. We want to develop a model to predict the median_house_value based on other variables in the dataset. The following code shows the first 5 rows of the dataset. . df = pd.read_csv(&#39;housing.csv&#39;) df.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity . 0 -122.23 | 37.88 | 41.0 | 880.0 | 129.0 | 322.0 | 126.0 | 8.3252 | 452600.0 | NEAR BAY | . 1 -122.22 | 37.86 | 21.0 | 7099.0 | 1106.0 | 2401.0 | 1138.0 | 8.3014 | 358500.0 | NEAR BAY | . 2 -122.24 | 37.85 | 52.0 | 1467.0 | 190.0 | 496.0 | 177.0 | 7.2574 | 352100.0 | NEAR BAY | . 3 -122.25 | 37.85 | 52.0 | 1274.0 | 235.0 | 558.0 | 219.0 | 5.6431 | 341300.0 | NEAR BAY | . 4 -122.25 | 37.85 | 52.0 | 1627.0 | 280.0 | 565.0 | 259.0 | 3.8462 | 342200.0 | NEAR BAY | . Preprocess Data . We rely on various functionalities provided by fastai to preprocess the data. We will not discuss them in detail here. For more information, please check this tutorial. One important note is that when dealing with categorical variables, instead of using one-hot encoding, we map each category into a distinct integer. . cont, cat = cont_cat_split(df, dep_var = &#39;median_house_value&#39;) . splits = RandomSplitter(valid_pct=0.2)(range_of(df)) . to = TabularPandas(df, procs=[Categorify, FillMissing,Normalize], cat_names = cat, cont_names = cont, y_names=&#39;median_house_value&#39;, splits=splits) . dls = to.dataloaders(bs=64) . Train Model . We will train a deep learning model to predict the median housing value and thus get the trained embedding for categorical variables. Again, see this tutorial for details about the meaning of the codes here. . learn = tabular_learner(dls, metrics=rmse) . early_stop_cb = EarlyStoppingCallback(patience=2) . learn.fit_one_cycle(10, cbs=early_stop_cb) . epoch train_loss valid_loss _rmse time . 0 | 55994937344.000000 | 56636186624.000000 | 237983.578125 | 00:04 | . 1 | 56567816192.000000 | 56058937344.000000 | 236767.671875 | 00:04 | . 2 | 52564221952.000000 | 52214001664.000000 | 228503.843750 | 00:03 | . 3 | 45972250624.000000 | 46276349952.000000 | 215119.375000 | 00:03 | . 4 | 37385846784.000000 | 37008117760.000000 | 192374.937500 | 00:03 | . 5 | 31154010112.000000 | 30351321088.000000 | 174216.312500 | 00:03 | . 6 | 24364992512.000000 | 23757260800.000000 | 154133.906250 | 00:03 | . 7 | 22229555200.000000 | 21549965312.000000 | 146799.031250 | 00:03 | . 8 | 20496922624.000000 | 21110308864.000000 | 145293.875000 | 00:03 | . 9 | 20571899904.000000 | 20511488000.000000 | 143218.312500 | 00:03 | . . Get Embedding . We will retrieve the trained embedding matrix from the model. . embs = [param for param in learn.model.embeds.parameters()] . The list has two elements and each element represents an embedding matrix for a categorical variable. . len(embs) . 2 . To check what each element corresponds to, we can use the cat_names attributes from TabularPandas. The list indicates that the first element in embs is the embedding matrix for the variable ocean_proximity. . to.cat_names . (#2) [&#39;ocean_proximity&#39;,&#39;total_bedrooms_na&#39;] . Let&#39;s see this matrix. Note that we convert it from a tensor array to a numpy array to make the operation later easier. . ocean_emb = embs[0].detach().numpy() ocean_emb . array([[ 0.00074246, 0.01322438, -0.00539126, 0.00066858], [-0.01550745, -0.02467075, 0.01956671, -0.01148492], [ 0.24266721, 0.35117084, -0.35875005, 0.36369336], [-0.05629548, -0.06088502, 0.08677642, -0.10355063], [-0.01115109, -0.02015582, 0.01277605, -0.00476734], [-0.01796132, -0.02396758, 0.02242134, -0.0143294 ]], dtype=float32) . Each row in the matrix above corresponds to one category in the categorical variable. The categories for ocean_proximity are the following. . cat = to.procs.categorify ocean_cat = cat[&#39;ocean_proximity&#39;] ocean_cat . [&#39;#na#&#39;, &#39;&lt;1H OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN&#39;] . We can use the o2i attribute to see how each category is mapped into an integer. The integer corresponds to the row in the embedding matrix. . ocean_cat.o2i . defaultdict(int, {&#39;#na#&#39;: 0, &#39;&lt;1H OCEAN&#39;: 1, &#39;INLAND&#39;: 2, &#39;ISLAND&#39;: 3, &#39;NEAR BAY&#39;: 4, &#39;NEAR OCEAN&#39;: 5}) . For example, the embedding for &#39;NEAR BAY&#39; is: . ocean_emb[4] . array([-0.01115109, -0.02015582, 0.01277605, -0.00476734], dtype=float32) . Let&#39;s create a dictionary to map each category to its corresponding embedding. . ocean_emb_map = {ocean_cat[i]:ocean_emb[i] for i in range(len(ocean_cat))} . Apply Embedding . This section shows how we can apply the embedding to the original dataset; that is, change the category variable to numeric vectors. . emb_dim = ocean_emb.shape[1] . col_name = [f&#39;ocean_emb_{i}&#39; for i in range(1,emb_dim+1)] df_emb = pd.DataFrame(df[&#39;ocean_proximity&#39;].map(ocean_emb_map).to_list(), columns=col_name) df_emb.head() . ocean_emb_1 ocean_emb_2 ocean_emb_3 ocean_emb_4 . 0 -0.011151 | -0.020156 | 0.012776 | -0.004767 | . 1 -0.011151 | -0.020156 | 0.012776 | -0.004767 | . 2 -0.011151 | -0.020156 | 0.012776 | -0.004767 | . 3 -0.011151 | -0.020156 | 0.012776 | -0.004767 | . 4 -0.011151 | -0.020156 | 0.012776 | -0.004767 | . df_new = pd.concat([df, df_emb],axis=1) . Visualize Embedding . Another way to explore the embedding matrix is to visualize it and see what it learns. We will use principle component analysis(PCA) to visualize the embedding. . from sklearn import decomposition pca = decomposition.PCA(n_components=2) pca_result = pca.fit_transform(ocean_emb) . df_visualize = pd.DataFrame({&#39;name&#39;:ocean_cat, &#39;dim1&#39;:pca_result[:,0], &#39;dim2&#39;: pca_result[:,1]}) sns.scatterplot(data=df_visualize, x=&#39;dim1&#39;, y=&#39;dim2&#39;, hue=&#39;name&#39;); . If we compare this visualization to the map of these houses above, we will find that the embedding matrix does provide some useful insights. In particular, we see that the relative location of the category INLAND does correspond to the inland area in the map. The code to make the following map comes from this notebook. . Ending Note . This is the end of this post. You can see the full code by clicking to the View on Github or Open in Colab tab at the top this post. .",
            "url": "https://wpan03.github.io/ds_blog/2020/12/21/fastai_emb.html",
            "relUrl": "/2020/12/21/fastai_emb.html",
            "date": " • Dec 21, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Logistic Regression's Coefficients Explanation",
            "content": "This post is going to introduce logistic regression with an emphasis on explaining the coefficients in the output of Python&#39;s statsmodels. . import numpy as np import pandas as pd import matplotlib.pyplot as plt . . 1. Background . Logistic regression is used to solve the classification problem. Classification problem means the dependent variable is categorical. For example, we can build a machine learning model to predict whether a student will be admitted to college based on metrics like his or her GPA, standardized test scores. If we formulate a classification problem in mathematical form, we have: . $$y = beta_{0} + beta_{1}x_{1} + beta_{2}x_{2}$$, . where y is a categorical variable, like whether you are admited or not. . One way to approach this problem is by using linear regression. However, we would like our algorithm will output a number between 0 and 1, which can indicate the probability of an observation belonging to a certain category. Linear regression does not satisfy this requirement as it might output values smaller than 0 or larger than 1. . One solution is to solve this problem is to transform the value of $ beta_{0} + beta_{1}x_{1} + beta_{2}x_{2}$ to the range of [0,1]. We can do this with the logistic function: $$f(x)= frac{e^{x}}{1+e^{x}}$$. . To show that logistic regression will make a number between 0 and 1. Let&#39;s make a plot of it. . def logit(x): return np.exp(x)/(1+np.exp(x)) . x = np.arange(-200, 200, 0.1) y = logit(x) plt.plot(x,y); . As we can see, the logit function has a S shape and is bounded by 0 and 1. When x is approaching positive infinity, the logit of x approaches 1. When x is approaching negative infinity, the logit of x approaches 0. . Thus, we can transform the value of $ beta_{0} + beta_{1}x_{1} + beta_{2}x_{2}$ to: . $$ p(X)= frac{e^{ beta_{0} + beta_{1}x_{1} + beta_{2}x_{2}}}{1+e^{ beta_{0} + beta_{1}x_{1} + beta_{2}x_{2}}} $$Now the output of our regression will fall between zero and one. Since we the logistic function to achieve this, the regression is called logistic regression. Before jumping into the details of behind it, let&#39;s first we how to run it in python. . 2. Python Implementation of Logistic Regression . For this post, we are going to use a college admission dataset from UCLA&#39;s Institute for Digital Research &amp; Education. We want to use a student&#39;s GRE score, GPA, and rank to predict whether the student will be admitted. . df = pd.read_csv(&quot;https://stats.idre.ucla.edu/stat/data/binary.csv&quot;) . df.head() . admit gre gpa rank . 0 0 | 380 | 3.61 | 3 | . 1 1 | 660 | 3.67 | 3 | . 2 1 | 800 | 4.00 | 1 | . 3 1 | 640 | 3.19 | 4 | . 4 0 | 520 | 2.93 | 4 | . We are going to use the statsmodel package to run the logistic regression. We choose statsmodel because its interface and summary of the result are more similar to other statistical software, like R and Stata. . import statsmodels.formula.api as smf . mod = smf.logit(formula=&#39;admit ~ gre + gpa + rank&#39;, data=df).fit() . Optimization terminated successfully. Current function value: 0.574302 Iterations 6 . . print(mod.summary()) . Logit Regression Results ============================================================================== Dep. Variable: admit No. Observations: 400 Model: Logit Df Residuals: 396 Method: MLE Df Model: 3 Date: Tue, 02 Feb 2021 Pseudo R-squ.: 0.08107 Time: 09:16:16 Log-Likelihood: -229.72 converged: True LL-Null: -249.99 Covariance Type: nonrobust LLR p-value: 8.207e-09 ============================================================================== coef std err z P&gt;|z| [0.025 0.975] Intercept -3.4495 1.133 -3.045 0.002 -5.670 -1.229 gre 0.0023 0.001 2.101 0.036 0.000 0.004 gpa 0.7770 0.327 2.373 0.018 0.135 1.419 rank -0.5600 0.127 -4.405 0.000 -0.809 -0.311 ============================================================================== . 3. Interpretation of the Result . 3.1 Prediction . Let&#39;s make a prediction for a student with GRE 300, GPA 2, and rank 4. . a_new_student = np.array([1, 300, 2, 4]) . We extract the parameters from the logistic regression model to make the prediction. . model_parameters = mod.params.values model_parameters . array([-3.44954840e+00, 2.29395950e-03, 7.77013574e-01, -5.60031387e-01]) . Note that the following calculation can be made much easier with matrix multiplication with code like a_new_student @ model_parameters. . pred = 0 for i in range(4): pred += model_parameters[i] * a_new_student[i] pred . -3.4474589462957206 . Note that the result is negative. Did we say in the previous section the logistic regression guarantees a prediction between 0 and 1? Then why do we get this negative prediction? To understand this, we need to understand the relationship between three terms - probability, odd, and log-odd. . 3.2 Probability, odd, log-odd . Let $k = beta_{0} + beta_{1}x_{1} + beta_{2}x_{2} + beta_{3}x_{3}$. As we discussed in section 1, we have: $p(X)= frac{e^{k}}{1+e^{k}}$, which is the probability of X belonging to a certain class. . Then, $p(x) + p(x)e^k = e^k$. Then $p(x) = e^k - p(x)e^k = e^k(1-p(x))$. Thus, $e^{k} = frac{p(x)}{1-p(x)}$. The quantity $ frac{p(x)}{1-p(x)}$ is called the odds. . Taking log from both side the odds equation, we have $k= log( frac{p(x)}{1-p(x)})$. This quantity $log( frac{p(x)}{1-p(x)})$ the log-odds. . Bringing back the value of k, we have: $log( frac{p(x)}{1-p(x)}) = beta_{0} + beta_{1}x_{1} + beta_{2}x_{2} + beta_{3}x_{3}$. . What we are actually evaluating when running the python code in the section is the equation above. Thus, the prediction we made is not the probability, but the log-odds. . To evaluate the probability, we can put the log-odds we predict back to the formula $p(X)= frac{e^{k}}{1+e^{k}}$, as k is the log-odd we predict. . In code, we have: . def prob_from_log_odd(log_odds): prob = np.exp(log_odds)/(1+np.exp(log_odds)) return prob . Using the prediction we have in section 3.1, we have: . print(&quot;The log odd is: &quot;, pred) print(&quot;The probability is: &quot;, prob_from_log_odd(pred)) . The log odd is: -3.4474589462957206 The probability is: 0.03084472943016926 . Note that the probability is about 0.03, which is between 0 and 1. To determine whether the student will be admitted or not, we usually set a 0.5 as the threshold. For students with predicted probability lower than 0.5, we will predict the result as reject. Otherwise, we will predict the result as admit. . 3.3 Coefficient Explanation . Based on the discussion in section 3.2, the explanation of a specific coefficient of the logistic regression is that given all other variables being the same, how much log-odd will change given one unit increase of the independent variable. For example, the coefficient for gre is about 0.0023, which can be interpreted as given all other variables being the same, one additional GRE score will lead to a 0.0023 increase of the log odd. . 4. Estimation Method . One question we did not answer so far is that how do we get the estimation of these coefficients for logistic regression. The estimation uses a statistical method called maximum likelihood estimation. The details for this method is beyond the scope of this post. The book The Elements of Statistical Learning gives a detailed discussion of the estimation process in section 4.4. A Python implementation of the estimation from scratch can be found in this post. .",
            "url": "https://wpan03.github.io/ds_blog/2020/11/20/logistic.html",
            "relUrl": "/2020/11/20/logistic.html",
            "date": " • Nov 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "What I wish I knew about R",
          "content": "As the title suggested, I want to share some ideas I learned from my experience of using R. The article has two parts: . Editor Tricks: this part discusses some keyboard shortcuts, rstudio tricks and rmarkdown tricks that hopefully can improve your productivity. . | Good Package: this part talks about some packages I really like. As usual, suggesting package can be opinionated. Those packages might also be outdated as the field of data science moves swiftly. However, this reflects my best understanding now and I hope it can at least bring some new ideas to you. . | . Editor Tricks . Keyboard Shortcut . Mastering the keyboard shortcut for a editor or IDE can be very helpful for productivity but also very challenging. So I want to highlight some of them that I find particular helpful. Note that I use a Mac and so I will write the shortcut’s Mac version. But the command should work once you replace cmd/control with crtl and Option with Alt. A full list of keyboard shortcut can be found here. . control + shift + 0: move R studio back to the usual four panes mode . | control + shift + 1: maximize the R script, notebook, or R Markdown file you are working in . | cmd + shift + c: comment / uncomment the current line . | cmd + option + i: insert a R chunk in Rmarkdown . | cmd + shift + enter: run the current code chunk in Rmarkdown . | cmd + shift + M: insert the pipe operator %&gt;% inside R code . | option + -: insert &lt;- inside R code . | control + shift + O: open/close document outline . | . Rstudio Tricks . These features discussed here are released in R studio 1.4. So to have these features, your R studio need to have at least version 1.4. The R studio blog did a great job to demonstrate these features and thus I will not repeat the demo here. Please do open the links in this section to check them out. . Visual Markdown Editing: with this feature, you can edit a markdown file in R studio and see the preview result for markdown immediately. Please see how to use it here. . | Citation Support: inserting citations in Rmarkdown becomes very easy with this feature. Check it out here. . | Rainbow Parentheses: this feature gives color to the those nested parentheses in R. See here. . | Outline mode in R script: this feature allows you to better organize your .R script. See how it works here. The keyboard shortcut control + shift + O can also open outline mode in R script. . | . Rmarkdown . From those features above, you might already feel that R Markdown is a very powerful and flexible document format. In fact, this document you are reading now is written with R Markdown. The book R Markdown: The Definitive Guide gives a detailed descriptions about many features of R markdown. I just want to highlight few sections I find particular helpful. . Markdown syntax: if you are not familiar with the basic syntax of markdown file, see this section. . | R chunk: one thing that is very powerful but can be confusing is the configuration of R chunk in R markdown. For example, you might see a code chunk below. What does eval and echo and many other possible words mean? This section discusses those. . | . {eval=TRUE, echo=TRUE} x &lt;- rnorm(100) . HTML Generation: there are many ways to customize the html file you generate from R markdown. Tricks I like include: generating table of content and displaying a dataframe beautifully. Check out this section for how to do those and many other things. | . There are a lot of other features we don’t highlight here, such as generating presentation and customizing the pdf output. Check out relevant sections in the book if you want to learn these. . Good Packages . Dplyr . Given the popularity of dplyr package, it seems unnecessary to advocate for it here. However, R provides several ways to manipulate dataframe, including the operation from Base R, dplyr and data.table and it might be hard to choose which one to master first. In addition to its intuitive syntax and nice integration of other parts of the tidyverse ecosystem, I want to give few other reasons to discuss why mastering dplyr first can be a good choice. . speed: usually, data.table excels at manipulating dataframe in a larger scale and having good computational performance. However, there is a package called dtplyr that allows you to use the intuitive syntax of dplyr and has almost same speed as data.table with very little change of syntax. . | spark: if you want to use Spark in R, the package sparklyr has very similar syntax as . | . Tidymodels . One thing I once found annoying about R is that you need to learn a lot of different packages and slightly different syntax to implement different machine learning models. In contrast, scikit-learn in Python provides a unified and consistent interface for different machine learning models. However, when I discover tidymodels pacakge, I feel that this might no longer be the case. Note that there are other efforts like caret to achieve the same thing. Personally, I find tidymodels easier to use. (Actually, caret and tidymodels are developed by the same author and tidymodels is the current focus of the author). . You can take a quick look of this document to get why tidymodels provides a consistent interface. To learn this package, this series of article provides a great introduction. There is also the free book Tidy Modeling with R that introduces how to do machine learning modeling in more details. . Plotly . The package ggplot2 plays a dominant role in data visualization world in R and it is indeed very beautifully designed. But I want to introduce another package called plotly that might be very helpful in doing visualization in R. . First, if you like ggplot2, there is a function called ggplotly that generates the output of ggplot in a interactive form. Consider the following example. . {r, message=FALSE} library(ggplot2) library(plotly) . fig &lt;- ggplot(mpg, aes(displ, hwy, colour = class)) + geom_point() ggplotly(fig) . In addition to that, the syntax and default display of `plotly` is also quite good. For example, we can do the scatter plot above with following code in `plotly`. {r, message=FALSE} plot_ly(data = mpg, x = ~displ, y = ~hwy, color = ~ class, type = &#39;scatter&#39;) . I think it is worthwhile to add plotly into your toolbox. . Endnotes . Thanks for reading this article and I hope it brings you some new ideas. I will try to update this article as I learn new things in my data science journey. .",
          "url": "https://wpan03.github.io/ds_blog/R_blog/R_tricks.Rmd",
          "relUrl": "/R_blog/R_tricks.Rmd",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://wpan03.github.io/ds_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://wpan03.github.io/ds_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}